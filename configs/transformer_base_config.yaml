data_configs:
  lang_pair: "zh-en"
  train_data:
    - "/home/user_data/weihr/NMT_DATA_PY3/1.34M/train/zh.under50.txt"
    - "/home/user_data/weihr/NMT_DATA_PY3/1.34M/train/en.under50.txt"
  valid_data:
    - "/home/user_data/weihr/NMT_DATA_PY3/1.34M/test/MT03/zh.0"
    - "/home/user_data/weihr/NMT_DATA_PY3/1.34M/test/MT03/en.0"
  bleu_valid_reference: "/home/user_data/weihr/NMT_DATA_PY3/1.34M/test/MT03/en."
  vocabularies:
    - type: "word"
      dict_path: "/home/user_data/weihr/NMT_DATA_PY3/1.34M/dict/dict.zh.json"
      max_n_words: 30000
    - type: "word"
      dict_path: "/home/user_data/weihr/NMT_DATA_PY3/1.34M/dict/dict.en.json"
      max_n_words: 30000
  max_len:
    - -1
    - -1
  num_refs: 4
  eval_at_char_level: false

model_configs:
  model: Transformer
  n_layers: 6
  n_head: 8
  d_word_vec: &dim 512
  d_model: *dim
  d_inner_hid: 2048
  dropout: 0.1
  proj_share_weight: true
  label_smoothing: 0.1

optimizer_configs:
  optimizer: "adam"
  learning_rate: 0.2
  grad_clip: -1.0
  optimizer_params:
    betas:
      - 0.9
      - 0.98
  schedule_method: noam
  scheduler_configs:
    d_model: *dim
    warmup_steps: 8000

training_configs:
  max_epochs: 1000000
  shuffle: true
  use_bucket: true
  batching_key: "tokens"
  batch_size: 1024
  update_cycle: 4
  valid_batch_size: 20
  bleu_valid_batch_size: 10
  bleu_valid_max_steps: 150
  bleu_valid_warmup: 1 # Start to do BLEU validation after those steps
  bleu_valid_configs:
    sacrebleu_args: "--tokenize none -lc"
    postprocess: false
  num_kept_checkpoints: 1
  disp_freq: 100 # Frequency to print information
  save_freq: 1000 # Frequency to save the model
  loss_valid_freq: 1000
  bleu_valid_freq: 1000
  early_stop_patience: 20
